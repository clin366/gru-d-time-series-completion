{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.pardir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from configparser import ExtendedInterpolation\n",
    "config_path = 'config/parameters.ini'\n",
    "pars = configparser.ConfigParser(interpolation=ExtendedInterpolation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config/parameters.ini']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del pars\n",
    "pars = configparser.ConfigParser(interpolation=ExtendedInterpolation())\n",
    "pars.read(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airpolnowcast.data.utils import read_query_from_file, read_raw_data\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airpolnowcast.features.build_features import FeatureEngineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from airpolnowcast.evaluation.utils import get_feature_pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_seq_path = 'data/raw/trend_seq.pkl'\n",
    "masking_seq_path = 'data/raw/masking_seq.pkl'\n",
    "delta_seq_path = 'data/raw/delta_seq.pkl'\n",
    "y_out_path = 'data/raw/y_out.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(trend_seq_path, 'rb') as fi:\n",
    "    trend_seq = pickle.load(fi)\n",
    "    \n",
    "with open(masking_seq_path, 'rb') as fi:\n",
    "    masking_seq = pickle.load(fi)\n",
    "    \n",
    "with open(delta_seq_path, 'rb') as fi:\n",
    "    delta_seq = pickle.load(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trend_seq[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean_path = 'data/raw/x_mean_aft_nor.pkl'\n",
    "x_median_path = 'data/raw/x_median_aft_nor.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(x_mean_path, 'rb') as fi:\n",
    "    x_mean_aft_nor = pickle.load(fi)\n",
    "\n",
    "with open(x_median_path, 'rb') as fi:\n",
    "    x_median_aft_nor = pickle.load(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(y_out_path, 'rb') as fi:\n",
    "    y_data = pickle.load(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y_data.reshape(len(y_data), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dataset = np.stack((trend_seq, masking_seq, delta_seq), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dataset = np.einsum('klij->klji', t_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dataset_path = 'data/raw/t_dataset.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3762531 , -0.3762531 , -0.3762531 , -0.3762531 , -0.3762531 ,\n",
       "        -0.3762531 , -0.3762531 ],\n",
       "       [-0.53141531, -0.53141531, -0.45528239, -0.45528239, -0.45528239,\n",
       "        -0.45528239, -0.64561468],\n",
       "       [ 0.32315053,  0.32315053,  0.32315053,  0.41780231,  0.41780231,\n",
       "        -0.00813072, -0.00813072],\n",
       "       [-1.1359439 , -0.34299963, -0.60731439, -0.64507364, -0.34299963,\n",
       "         1.20512965, -1.0604254 ],\n",
       "       [-0.53610029, -0.67433154, -0.65458422, -0.65458422, -0.65458422,\n",
       "        -0.65458422, -0.65458422],\n",
       "       [-0.46189194, -0.46189194, -0.46189194, -0.00730524, -0.30278659,\n",
       "        -0.30278659, -0.18913992],\n",
       "       [-0.30408974,  0.12893314,  1.4280018 ,  1.39191656,  0.30935935,\n",
       "         1.10323464,  0.20110363],\n",
       "       [ 0.37286645,  0.37286645,  0.37286645,  0.37286645,  0.37286645,\n",
       "         0.37286645,  0.37286645],\n",
       "       [ 0.05068576, -0.88974464, -0.85615784, -0.88974464, -0.08366144,\n",
       "        -0.08366144, -0.65463704],\n",
       "       [ 2.02864697,  2.02864697,  2.02864697,  2.02864697,  0.87889969,\n",
       "         0.87889969,  1.10884915],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.41366219, -0.41366219, -0.41366219, -0.41366219, -0.41366219,\n",
       "        -0.41366219, -0.41366219],\n",
       "       [-0.55616942, -0.55616942, -0.55616942, -0.23539186,  2.17043984,\n",
       "         2.17043984,  2.17043984],\n",
       "       [-0.32421539,  0.44790317, -0.59277836, -0.59277836, -0.76063022,\n",
       "        -0.7942006 , -0.72705985],\n",
       "       [-0.15945658, -0.1111191 , -0.1111191 ,  0.61394314,  0.13056832,\n",
       "         0.13056832,  2.69245489],\n",
       "       [ 0.67972935,  0.87679925,  1.20524908,  0.38412451,  0.64688437,\n",
       "         1.50085393,  0.35127952],\n",
       "       [-0.92569714, -0.09441927, -0.81631847, -0.6413126 , -0.33505234,\n",
       "        -0.48818247, -0.75069127],\n",
       "       [ 0.01449693,  0.01449693,  0.01449693,  0.01449693,  0.01449693,\n",
       "         0.01449693,  0.01449693],\n",
       "       [-1.10504749, -0.45710868, -0.66327103, -0.69272279, -0.25094633,\n",
       "        -0.83998161, -0.6043675 ],\n",
       "       [-0.87284713,  1.81991986,  0.47353637,  0.25520391,  1.41964369,\n",
       "        -0.61812592, -0.54534844],\n",
       "       [ 2.60223204,  0.55619343,  0.55619343,  0.55619343,  0.55619343,\n",
       "         2.03124452,  2.03124452],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.9994896 , -0.9994896 ,  0.00883911, -0.88086269,  1.106138  ,\n",
       "        -0.9994896 , -0.9994896 ],\n",
       "       [-0.29476131, -0.29476131, -0.29476131, -0.29476131, -0.29476131,\n",
       "        -0.29476131, -0.29476131],\n",
       "       [-0.31969482, -0.31969482, -1.52586261, -1.54779293, -1.54779293,\n",
       "        -1.54779293, -1.54779293],\n",
       "       [-0.45612937, -0.60328075, -0.11277613,  0.08342572,  0.86823313,\n",
       "         0.27962757, -0.35802844],\n",
       "       [-0.3117616 , -0.3117616 , -0.3117616 , -0.3117616 , -0.35105412,\n",
       "        -0.35105412, -0.27246909],\n",
       "       [ 0.11987584,  0.11987584,  0.11987584,  0.11987584,  0.11987584,\n",
       "         0.11987584,  0.11987584],\n",
       "       [-0.25107157, -0.25107157, -0.25107157, -0.25107157, -0.27183455,\n",
       "        -0.27183455, -0.27183455],\n",
       "       [-0.47969512, -0.47969512, -0.44115488, -0.44115488, -0.67239637,\n",
       "        -0.13283288, -0.59531587],\n",
       "       [-0.39077173, -0.39077173, -0.39077173, -0.39077173, -0.39077173,\n",
       "        -0.39077173, -0.39077173],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.67893996,  0.67893996,  0.67893996,  0.67893996,  0.67893996,\n",
       "         0.67893996,  0.67893996],\n",
       "       [-0.67624194, -0.66577335, -0.66577335, -0.66577335, -0.66577335,\n",
       "        -0.66577335, -0.66577335],\n",
       "       [-0.27460339, -0.27460339, -0.26410702, -0.26527329, -0.26527329,\n",
       "        -0.26527329, -0.26527329],\n",
       "       [-0.25429388, -0.25429388, -0.25429388, -0.25429388, -0.25429388,\n",
       "        -0.25429388, -0.25429388],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.39004102, -0.39004102, -0.39004102, -0.39004102, -0.39004102,\n",
       "        -0.39004102, -0.39004102],\n",
       "       [-0.52754429, -0.96109572, -0.96109572, -0.96109572, -0.03979893,\n",
       "        -1.06948357, -0.491415  ],\n",
       "       [-0.44626425, -0.19558776, -0.19558776,  1.55914766, -0.52982308,\n",
       "         0.97423585, -0.40448483],\n",
       "       [-0.21160912,  0.05187204,  0.0895122 ,  0.0895122 ,  0.0895122 ,\n",
       "         0.0895122 ,  0.0895122 ],\n",
       "       [ 0.54737605,  0.54737605,  0.54737605,  0.54737605,  0.54737605,\n",
       "         0.54737605,  0.54737605],\n",
       "       [ 2.33986132,  1.23437057,  1.63636721,  1.23437057,  1.13387141,\n",
       "         1.38511931,  1.48561847],\n",
       "       [-0.55379871, -0.69940497,  1.73949981, -0.69940497, -0.88141279,\n",
       "         0.10142944,  1.22987792],\n",
       "       [-0.61850909, -0.61850909, -0.61850909, -0.61850909, -0.61850909,\n",
       "        -0.63596286, -0.63596286],\n",
       "       [-0.7580527 , -1.2233135 , -1.19229612, -1.2233135 , -1.06822657,\n",
       "         0.79281665, -0.72703531],\n",
       "       [-0.45033884, -0.45033884, -0.45033884, -0.45033884, -0.45033884,\n",
       "        -0.31173262, -0.31173262],\n",
       "       [-0.52578567, -0.52578567, -0.52578567, -0.52578567, -0.52578567,\n",
       "        -0.52578567, -0.52578567],\n",
       "       [ 0.49287074,  4.23105636,  3.01614603,  3.57687388,  3.01614603,\n",
       "         3.76378316,  4.13760172],\n",
       "       [-0.52934026,  0.80419797, -0.44043772,  0.18188013, -0.67751118,\n",
       "        -0.67751118, -0.67751118],\n",
       "       [ 2.88060745,  2.88060745,  2.88060745,  2.88060745,  2.88060745,\n",
       "         2.88060745,  2.88060745]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_dataset[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(t_dataset_path, 'wb') as fo:\n",
    "    pickle.dump(t_dataset, fo)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_dataloader(dataset, outcomes,\\\n",
    "                    train_proportion = 0.8, dev_proportion = 0.2, test_proportion = 0.2):\n",
    "    \n",
    "    train_index = int(np.floor(dataset.shape[0] * train_proportion))\n",
    "    dev_index = int(np.floor(dataset.shape[0] * (train_proportion - dev_proportion)))\n",
    "    \n",
    "    # split dataset to tarin/dev/test set\n",
    "    train_data, train_label = dataset[:train_index, :,:,:], outcomes[:train_index, :]\n",
    "    dev_data, dev_label = dataset[dev_index:train_index, :,:,:], outcomes[dev_index:train_index, :]\n",
    "    test_data, test_label = dataset[train_index:, :,:,:], outcomes[train_index:, :]   \n",
    "    \n",
    "    # ndarray to tensor\n",
    "    train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label)\n",
    "    dev_data, dev_label = torch.Tensor(dev_data), torch.Tensor(dev_label)\n",
    "    test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label)\n",
    "    \n",
    "    # tensor to dataset\n",
    "    train_dataset = utils.TensorDataset(train_data, train_label)\n",
    "    dev_dataset = utils.TensorDataset(dev_data, dev_label)\n",
    "    test_dataset = utils.TensorDataset(test_data, test_label)\n",
    "    \n",
    "    # dataset to dataloader \n",
    "    train_dataloader = utils.DataLoader(train_dataset)\n",
    "    dev_dataloader = utils.DataLoader(dev_dataset)\n",
    "    test_dataloader = utils.DataLoader(test_dataset)\n",
    "    \n",
    "    print(\"train_data.shape : {}\\t train_label.shape : {}\".format(train_data.shape, train_label.shape))\n",
    "    print(\"dev_data.shape : {}\\t dev_label.shape : {}\".format(dev_data.shape, dev_label.shape))\n",
    "    print(\"test_data.shape : {}\\t test_label.shape : {}\".format(test_data.shape, test_label.shape))\n",
    "    \n",
    "    return train_dataloader, dev_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape : torch.Size([2337, 3, 51, 7])\t train_label.shape : torch.Size([2337, 1])\n",
      "dev_data.shape : torch.Size([584, 3, 51, 7])\t dev_label.shape : torch.Size([584, 1])\n",
      "test_data.shape : torch.Size([585, 3, 51, 7])\t test_label.shape : torch.Size([585, 1])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, dev_dataloader, test_dataloader = data_dataloader(t_dataset, \n",
    "                                                                    y_data, train_proportion=0.8, dev_proportion=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, criterion, learning_rate,\\\n",
    "        train_dataloader, dev_dataloader, test_dataloader,\\\n",
    "        learning_rate_decay=0, n_epochs=30):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    # to check the update \n",
    "    old_state_dict = {}\n",
    "    for key in model.state_dict():\n",
    "        old_state_dict[key] = model.state_dict()[key].clone()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        if learning_rate_decay != 0:\n",
    "\n",
    "            # every [decay_step] epoch reduce the learning rate by half\n",
    "            if  epoch % learning_rate_decay == 0:\n",
    "                learning_rate = learning_rate/2\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                print('at epoch {} learning_rate is updated to {}'.format(epoch, learning_rate))\n",
    "        \n",
    "        # train the model\n",
    "        losses, acc = [], []\n",
    "        label, pred = [], []\n",
    "        y_pred_col= []\n",
    "        model.train()\n",
    "        for train_data, train_label in train_dataloader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Squeeze the data [1, 33, 49], [1,5] to [33, 49], [5]\n",
    "            train_data = torch.squeeze(train_data)\n",
    "            train_label = torch.squeeze(train_label)\n",
    "            \n",
    "            # Forward pass : Compute predicted y by passing train data to the model\n",
    "            y_pred = model(train_data)\n",
    "            \n",
    "            # y_pred = y_pred[:, None]\n",
    "            # train_label = train_label[:, None]\n",
    "            \n",
    "            #print(y_pred.shape)\n",
    "            #print(train_label.shape)\n",
    "            \n",
    "            # Save predict and label\n",
    "            y_pred_col.append(y_pred.item())\n",
    "            pred.append(y_pred.item() > 0.5)\n",
    "            label.append(train_label.item())\n",
    "            \n",
    "            #print('y_pred: {}\\t label: {}'.format(y_pred, train_label))\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(y_pred, train_label)\n",
    "            acc.append(\n",
    "                torch.eq(\n",
    "                    (torch.sigmoid(y_pred).data > 0.5).float(),\n",
    "                    train_label)\n",
    "            )\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # perform a backward pass, and update the weights.\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        \n",
    "        train_acc = torch.mean(torch.cat(acc).float())\n",
    "        train_loss = np.mean(losses)\n",
    "        \n",
    "        train_pred_out = pred\n",
    "        train_label_out = label\n",
    "        \n",
    "        # save new params\n",
    "        new_state_dict= {}\n",
    "        for key in model.state_dict():\n",
    "            new_state_dict[key] = model.state_dict()[key].clone()\n",
    "            \n",
    "        # compare params\n",
    "        for key in old_state_dict:\n",
    "            if (old_state_dict[key] == new_state_dict[key]).all():\n",
    "                print('Not updated in {}'.format(key))\n",
    "   \n",
    "        \n",
    "        # dev loss\n",
    "        losses, acc = [], []\n",
    "        label, pred = [], []\n",
    "        model.eval()\n",
    "        for dev_data, dev_label in dev_dataloader:\n",
    "            # Squeeze the data [1, 33, 49], [1,5] to [33, 49], [5]\n",
    "            dev_data = torch.squeeze(dev_data)\n",
    "            dev_label = torch.squeeze(dev_label)\n",
    "            \n",
    "            # Forward pass : Compute predicted y by passing train data to the model\n",
    "            y_pred = model(dev_data)\n",
    "            \n",
    "            # Save predict and label\n",
    "            pred.append(y_pred.item())\n",
    "            label.append(dev_label.item())\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(y_pred, dev_label)\n",
    "            acc.append(\n",
    "                torch.eq(\n",
    "                    (torch.sigmoid(y_pred).data > 0.5).float(),\n",
    "                    dev_label)\n",
    "            )\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "        dev_acc = torch.mean(torch.cat(acc).float())\n",
    "        dev_loss = np.mean(losses)\n",
    "        \n",
    "        dev_pred_out = pred\n",
    "        dev_label_out = label\n",
    "        \n",
    "        # test loss\n",
    "        losses, acc = [], []\n",
    "        label, pred = [], []\n",
    "        model.eval()\n",
    "        for test_data, test_label in test_dataloader:\n",
    "            # Squeeze the data [1, 33, 49], [1,5] to [33, 49], [5]\n",
    "            test_data = torch.squeeze(test_data)\n",
    "            test_label = torch.squeeze(test_label)\n",
    "            \n",
    "            # Forward pass : Compute predicted y by passing train data to the model\n",
    "            y_pred = model(test_data)\n",
    "            \n",
    "            # Save predict and label\n",
    "            pred.append(y_pred.item())\n",
    "            label.append(test_label.item())\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(y_pred, test_label)\n",
    "            acc.append(\n",
    "                torch.eq(\n",
    "                    (torch.sigmoid(y_pred).data > 0.5).float(),\n",
    "                    test_label)\n",
    "            )\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "        test_acc = torch.mean(torch.cat(acc).float())\n",
    "        test_loss = np.mean(losses)\n",
    "        \n",
    "        test_pred_out = pred\n",
    "        test_label_out = label\n",
    "                \n",
    "        epoch_losses.append([\n",
    "             train_loss, dev_loss, test_loss,\n",
    "             train_acc, dev_acc, test_acc,\n",
    "             train_pred_out, dev_pred_out, test_pred_out,\n",
    "             train_label_out, dev_label_out, test_label_out,\n",
    "         ])\n",
    "        \n",
    "        pred = np.asarray(pred)\n",
    "        label = np.asarray(label)\n",
    "        \n",
    "        auc_score = roc_auc_score(label, pred)\n",
    "        \n",
    "        # print(\"Epoch: {} Train: {:.4f}/{:.2f}%, Dev: {:.4f}/{:.2f}%, Test: {:.4f}/{:.2f}% AUC: {:.4f}\".format(\n",
    "        #     epoch, train_loss, train_acc*100, dev_loss, dev_acc*100, test_loss, test_acc*100, auc_score))\n",
    "        print(\"Epoch: {} Train loss: {:.4f}, Dev loss: {:.4f}, Test loss: {:.4f}, Test AUC: {:.4f}\".format(\n",
    "            epoch, train_loss, dev_loss, test_loss, auc_score))\n",
    "        \n",
    "        # save the parameters\n",
    "        train_log = []\n",
    "        train_log.append(model.state_dict())\n",
    "        torch.save(model.state_dict(), './data/raw/grud_mean_grud_para.pt')\n",
    "        \n",
    "        #print(train_log)\n",
    "    \n",
    "    return epoch_losses      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2922, 3, 51, 7)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 51 # num of variables base on the paper\n",
    "hidden_size = 51 # same as inputsize\n",
    "output_size = 1\n",
    "num_layers = 7 # num of step or layers base on the paper\n",
    "\n",
    "# x_mean = torch.Tensor(np.load('./input/x_mean_aft_nor.npy'))\n",
    "# x_median = torch.Tensor(np.load('./input/x_median_aft_nor.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropout_type : Moon, Gal, mloss\n",
    "a_model = GRUD(input_size = input_size, hidden_size= hidden_size, output_size=output_size, dropout=0, dropout_type='mloss', x_mean=x_mean_aft_nor, num_layers=num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "learning_rate_decay =7 \n",
    "n_epochs = 14\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 0 learning_rate is updated to 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train loss: 0.4503, Dev loss: 0.3872, Test loss: 0.3807, Test AUC: 0.5461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.4328, Dev loss: 0.3242, Test loss: 0.3814, Test AUC: 0.5194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train loss: 0.4291, Dev loss: 0.3201, Test loss: 0.3833, Test AUC: 0.4990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train loss: 0.4251, Dev loss: 0.3085, Test loss: 0.3942, Test AUC: 0.4792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train loss: 0.4202, Dev loss: 0.2868, Test loss: 0.3936, Test AUC: 0.4649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train loss: 0.4182, Dev loss: 0.2748, Test loss: 0.4008, Test AUC: 0.4558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Train loss: 0.4172, Dev loss: 0.2688, Test loss: 0.4096, Test AUC: 0.4522\n",
      "at epoch 7 learning_rate is updated to 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Train loss: 0.4228, Dev loss: 0.2634, Test loss: 0.4191, Test AUC: 0.4476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Train loss: 0.4094, Dev loss: 0.2587, Test loss: 0.4188, Test AUC: 0.4448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Train loss: 0.4018, Dev loss: 0.2545, Test loss: 0.4214, Test AUC: 0.4464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Train loss: 0.3956, Dev loss: 0.2510, Test loss: 0.4293, Test AUC: 0.4432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Train loss: 0.3925, Dev loss: 0.2464, Test loss: 0.4295, Test AUC: 0.4423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Train loss: 0.3872, Dev loss: 0.2431, Test loss: 0.4325, Test AUC: 0.4414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Train loss: 0.3837, Dev loss: 0.2411, Test loss: 0.4349, Test AUC: 0.4410\n"
     ]
    }
   ],
   "source": [
    "# learning_rate = 0.1 learning_rate_decay=True\n",
    "epoch_losses = fit(a_model, criterion, learning_rate,\\\n",
    "                   train_dataloader, dev_dataloader, test_dataloader,\\\n",
    "                   learning_rate_decay, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "class GRUD(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, x_mean=0,\\\n",
    "                 bias=True, batch_first=False, bidirectional=False, dropout_type='mloss', dropout=0):\n",
    "        super(GRUD, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.zeros = torch.autograd.Variable(torch.zeros(input_size))\n",
    "        self.x_mean = torch.autograd.Variable(torch.tensor(x_mean))\n",
    "        self.bias = bias\n",
    "        self.batch_first = batch_first\n",
    "        self.dropout_type = dropout_type\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        if not isinstance(dropout, numbers.Number) or not 0 <= dropout <= 1 or \\\n",
    "                isinstance(dropout, bool):\n",
    "            raise ValueError(\"dropout should be a number in range [0, 1] \"\n",
    "                             \"representing the probability of an element being \"\n",
    "                             \"zeroed\")\n",
    "        if dropout > 0 and num_layers == 1:\n",
    "            warnings.warn(\"dropout option adds dropout after all but last \"\n",
    "                          \"recurrent layer, so non-zero dropout expects \"\n",
    "                          \"num_layers greater than 1, but got dropout={} and \"\n",
    "                          \"num_layers={}\".format(dropout, num_layers))\n",
    "        \n",
    "        ################################\n",
    "        gate_size = 1 # not used\n",
    "        ################################\n",
    "        \n",
    "        self._all_weights = []\n",
    "\n",
    "        '''\n",
    "        w_ih = Parameter(torch.Tensor(gate_size, layer_input_size))\n",
    "        w_hh = Parameter(torch.Tensor(gate_size, hidden_size))\n",
    "        b_ih = Parameter(torch.Tensor(gate_size))\n",
    "        b_hh = Parameter(torch.Tensor(gate_size))\n",
    "        layer_params = (w_ih, w_hh, b_ih, b_hh)\n",
    "        '''\n",
    "        # decay rates gamma\n",
    "        w_dg_x = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "        w_dg_h = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # z\n",
    "        w_xz = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "        w_hz = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        w_mz = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "\n",
    "        # r\n",
    "        w_xr = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "        w_hr = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        w_mr = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "\n",
    "        # h_tilde\n",
    "        w_xh = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "        w_hh = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        w_mh = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "\n",
    "        # y (output)\n",
    "        w_hy = torch.nn.Parameter(torch.Tensor(output_size, hidden_size))\n",
    "\n",
    "        # bias\n",
    "        b_dg_x = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b_dg_h = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b_z = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b_r = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b_h = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b_y = torch.nn.Parameter(torch.Tensor(output_size))\n",
    "\n",
    "        layer_params = (w_dg_x, w_dg_h,\\\n",
    "                        w_xz, w_hz, w_mz,\\\n",
    "                        w_xr, w_hr, w_mr,\\\n",
    "                        w_xh, w_hh, w_mh,\\\n",
    "                        w_hy,\\\n",
    "                        b_dg_x, b_dg_h, b_z, b_r, b_h, b_y)\n",
    "\n",
    "        param_names = ['weight_dg_x', 'weight_dg_h',\\\n",
    "                       'weight_xz', 'weight_hz','weight_mz',\\\n",
    "                       'weight_xr', 'weight_hr','weight_mr',\\\n",
    "                       'weight_xh', 'weight_hh','weight_mh',\\\n",
    "                       'weight_hy']\n",
    "        if bias:\n",
    "            param_names += ['bias_dg_x', 'bias_dg_h',\\\n",
    "                            'bias_z',\\\n",
    "                            'bias_r',\\\n",
    "                            'bias_h',\\\n",
    "                            'bias_y']\n",
    "        \n",
    "        for name, param in zip(param_names, layer_params):\n",
    "            setattr(self, name, param)\n",
    "        self._all_weights.append(param_names)\n",
    "\n",
    "        self.flatten_parameters()\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def flatten_parameters(self):\n",
    "        \"\"\"\n",
    "        Resets parameter data pointer so that they can use faster code paths.\n",
    "        Right now, this works only if the module is on the GPU and cuDNN is enabled.\n",
    "        Otherwise, it's a no-op.\n",
    "        \"\"\"\n",
    "        any_param = next(self.parameters()).data\n",
    "        if not any_param.is_cuda or not torch.backends.cudnn.is_acceptable(any_param):\n",
    "            return\n",
    "\n",
    "        # If any parameters alias, we fall back to the slower, copying code path. This is\n",
    "        # a sufficient check, because overlapping parameter buffers that don't completely\n",
    "        # alias would break the assumptions of the uniqueness check in\n",
    "        # Module.named_parameters().\n",
    "        all_weights = self._flat_weights\n",
    "        unique_data_ptrs = set(p.data_ptr() for p in all_weights)\n",
    "        if len(unique_data_ptrs) != len(all_weights):\n",
    "            return\n",
    "\n",
    "        with torch.cuda.device_of(any_param):\n",
    "            import torch.backends.cudnn.rnn as rnn\n",
    "\n",
    "            # NB: This is a temporary hack while we still don't have Tensor\n",
    "            # bindings for ATen functions\n",
    "            with torch.no_grad():\n",
    "                # NB: this is an INPLACE function on all_weights, that's why the\n",
    "                # no_grad() is necessary.\n",
    "                torch._cudnn_rnn_flatten_weight(\n",
    "                    all_weights, (4 if self.bias else 2),\n",
    "                    self.input_size, rnn.get_cudnn_mode(self.mode), self.hidden_size, self.num_layers,\n",
    "                    self.batch_first, bool(self.bidirectional))\n",
    "\n",
    "    def _apply(self, fn):\n",
    "        ret = super(GRUD, self)._apply(fn)\n",
    "        self.flatten_parameters()\n",
    "        return ret\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            torch.nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def check_forward_args(self, input, hidden, batch_sizes):\n",
    "        is_input_packed = batch_sizes is not None\n",
    "        expected_input_dim = 2 if is_input_packed else 3\n",
    "        if input.dim() != expected_input_dim:\n",
    "            raise RuntimeError(\n",
    "                'input must have {} dimensions, got {}'.format(\n",
    "                    expected_input_dim, input.dim()))\n",
    "        if self.input_size != input.size(-1):\n",
    "            raise RuntimeError(\n",
    "                'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n",
    "                    self.input_size, input.size(-1)))\n",
    "\n",
    "        if is_input_packed:\n",
    "            mini_batch = int(batch_sizes[0])\n",
    "        else:\n",
    "            mini_batch = input.size(0) if self.batch_first else input.size(1)\n",
    "\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        expected_hidden_size = (self.num_layers * num_directions,\n",
    "                                mini_batch, self.hidden_size)\n",
    "        \n",
    "        def check_hidden_size(hx, expected_hidden_size, msg='Expected hidden size {}, got {}'):\n",
    "            if tuple(hx.size()) != expected_hidden_size:\n",
    "                raise RuntimeError(msg.format(expected_hidden_size, tuple(hx.size())))\n",
    "\n",
    "        if self.mode == 'LSTM':\n",
    "            check_hidden_size(hidden[0], expected_hidden_size,\n",
    "                              'Expected hidden[0] size {}, got {}')\n",
    "            check_hidden_size(hidden[1], expected_hidden_size,\n",
    "                              'Expected hidden[1] size {}, got {}')\n",
    "        else:\n",
    "            check_hidden_size(hidden, expected_hidden_size)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        s = '{input_size}, {hidden_size}'\n",
    "        if self.num_layers != 1:\n",
    "            s += ', num_layers={num_layers}'\n",
    "        if self.bias is not True:\n",
    "            s += ', bias={bias}'\n",
    "        if self.batch_first is not False:\n",
    "            s += ', batch_first={batch_first}'\n",
    "        if self.dropout != 0:\n",
    "            s += ', dropout={dropout}'\n",
    "        if self.bidirectional is not False:\n",
    "            s += ', bidirectional={bidirectional}'\n",
    "        return s.format(**self.__dict__)\n",
    "    \n",
    "    \n",
    "    def __setstate__(self, d):\n",
    "        super(GRUD, self).__setstate__(d)\n",
    "        if 'all_weights' in d:\n",
    "            self._all_weights = d['all_weights']\n",
    "        if isinstance(self._all_weights[0][0], str):\n",
    "            return\n",
    "        num_layers = self.num_layers\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        self._all_weights = []\n",
    "\n",
    "        weights = ['weight_dg_x', 'weight_dg_h',\\\n",
    "                   'weight_xz', 'weight_hz','weight_mz',\\\n",
    "                   'weight_xr', 'weight_hr','weight_mr',\\\n",
    "                   'weight_xh', 'weight_hh','weight_mh',\\\n",
    "                   'weight_hy',\\\n",
    "                   'bias_dg_x', 'bias_dg_h',\\\n",
    "                   'bias_z', 'bias_r', 'bias_h','bias_y']\n",
    "\n",
    "        if self.bias:\n",
    "            self._all_weights += [weights]\n",
    "        else:\n",
    "            self._all_weights += [weights[:2]]\n",
    "\n",
    "    @property\n",
    "    def _flat_weights(self):\n",
    "        return list(self._parameters.values())\n",
    "\n",
    "    @property\n",
    "    def all_weights(self):\n",
    "        return [[getattr(self, weight) for weight in weights] for weights in self._all_weights]\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # input.size = (3, 33,49) : num_input or num_hidden, num_layer or step\n",
    "        X = torch.squeeze(input[0]) # .size = (33,49)\n",
    "        Mask = torch.squeeze(input[1]) # .size = (33,49)\n",
    "        Delta = torch.squeeze(input[2]) # .size = (33,49)\n",
    "        Hidden_State = torch.autograd.Variable(torch.zeros(input_size))\n",
    "        \n",
    "        step_size = X.size(1) # 49\n",
    "        #print('step size : ', step_size)\n",
    "        \n",
    "        output = None\n",
    "        h = Hidden_State\n",
    "\n",
    "        # decay rates gamma\n",
    "        w_dg_x = getattr(self, 'weight_dg_x')\n",
    "        w_dg_h = getattr(self, 'weight_dg_h')\n",
    "\n",
    "        #z\n",
    "        w_xz = getattr(self, 'weight_xz')\n",
    "        w_hz = getattr(self, 'weight_hz')\n",
    "        w_mz = getattr(self, 'weight_mz')\n",
    "\n",
    "        # r\n",
    "        w_xr = getattr(self, 'weight_xr')\n",
    "        w_hr = getattr(self, 'weight_hr')\n",
    "        w_mr = getattr(self, 'weight_mr')\n",
    "\n",
    "        # h_tilde\n",
    "        w_xh = getattr(self, 'weight_xh')\n",
    "        w_hh = getattr(self, 'weight_hh')\n",
    "        w_mh = getattr(self, 'weight_mh')\n",
    "\n",
    "        # bias\n",
    "        b_dg_x = getattr(self, 'bias_dg_x')\n",
    "        b_dg_h = getattr(self, 'bias_dg_h')\n",
    "        b_z = getattr(self, 'bias_z')\n",
    "        b_r = getattr(self, 'bias_r')\n",
    "        b_h = getattr(self, 'bias_h')\n",
    "        \n",
    "        for layer in range(num_layers):\n",
    "            \n",
    "            x = torch.squeeze(X[:,layer:layer+1])\n",
    "            m = torch.squeeze(Mask[:,layer:layer+1])\n",
    "            d = torch.squeeze(Delta[:,layer:layer+1])\n",
    "\n",
    "\n",
    "            #(4)\n",
    "            gamma_x = torch.exp(-torch.max(self.zeros, (w_dg_x * d + b_dg_x)))\n",
    "            gamma_h = torch.exp(-torch.max(self.zeros, (w_dg_h * d + b_dg_h)))\n",
    "\n",
    "            #(5)\n",
    "            x = m * x + (1 - m) * (gamma_x * x + (1 - gamma_x) * self.x_mean)\n",
    "\n",
    "            #(6)\n",
    "            if self.dropout == 0:\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "\n",
    "            elif self.dropout_type == 'Moon':\n",
    "                '''\n",
    "                RNNDROP: a novel dropout for rnn in asr(2015)\n",
    "                '''\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "                dropout = torch.nn.Dropout(p=self.dropout)\n",
    "                h = dropout(h)\n",
    "\n",
    "            elif self.dropout_type == 'Gal':\n",
    "                '''\n",
    "                A Theoretically grounded application of dropout in recurrent neural networks(2015)\n",
    "                '''\n",
    "                dropout = torch.nn.Dropout(p=self.dropout)\n",
    "                h = dropout(h)\n",
    "\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "\n",
    "            elif self.dropout_type == 'mloss':\n",
    "                '''\n",
    "                recurrent dropout without memory loss arXiv 1603.05118\n",
    "                g = h_tilde, p = the probability to not drop a neuron\n",
    "                '''\n",
    "\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                dropout = torch.nn.Dropout(p=self.dropout)\n",
    "                h_tilde = dropout(h_tilde)\n",
    "\n",
    "                h = (1 - z)* h + z*h_tilde\n",
    "\n",
    "            else:\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "\n",
    "            \n",
    "        w_hy = getattr(self, 'weight_hy')\n",
    "        b_y = getattr(self, 'bias_y')\n",
    "\n",
    "        output = torch.matmul(w_hy.float(), h.float()) + b_y.float()\n",
    "        output = torch.sigmoid(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_median_aft_nor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2922, 7, 51)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masking_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2922, 7, 51)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_seq.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:airpol] *",
   "language": "python",
   "name": "conda-env-airpol-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

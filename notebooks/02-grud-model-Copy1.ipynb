{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import configparser\n",
    "from configparser import ExtendedInterpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from airpolnowcast.data.utils import read_query_from_file, read_raw_data\n",
    "import ast\n",
    "from airpolnowcast.features.build_features import FeatureEngineer\n",
    "from airpolnowcast.evaluation.utils import get_feature_pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_dataloader(dataset_train, outcomes_train, dataset_valid, outcomes_valid,\n",
    "                   dataset_test, outcomes_test):\n",
    "    \n",
    "    # split dataset to tarin/dev/test set\n",
    "    train_data, train_label = dataset_train, outcomes_train\n",
    "    dev_data, dev_label = dataset_valid, outcomes_valid\n",
    "    test_data, test_label = dataset_test, outcomes_test  \n",
    "    \n",
    "    # ndarray to tensor\n",
    "    train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label)\n",
    "    dev_data, dev_label = torch.Tensor(dev_data), torch.Tensor(dev_label)\n",
    "    test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label)\n",
    "    \n",
    "    # tensor to dataset\n",
    "    train_dataset = utils.TensorDataset(train_data, train_label)\n",
    "    dev_dataset = utils.TensorDataset(dev_data, dev_label)\n",
    "    test_dataset = utils.TensorDataset(test_data, test_label)\n",
    "    \n",
    "    # dataset to dataloader \n",
    "    train_dataloader = utils.DataLoader(train_dataset)\n",
    "    dev_dataloader = utils.DataLoader(dev_dataset)\n",
    "    test_dataloader = utils.DataLoader(test_dataset)\n",
    "    \n",
    "    print(\"train_data.shape : {}\\t train_label.shape : {}\".format(train_data.shape, train_label.shape))\n",
    "    print(\"dev_data.shape : {}\\t dev_label.shape : {}\".format(dev_data.shape, dev_label.shape))\n",
    "    print(\"test_data.shape : {}\\t test_label.shape : {}\".format(test_data.shape, test_label.shape))\n",
    "    \n",
    "    return train_dataloader, dev_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, criterion, learning_rate,\\\n",
    "        train_dataloader, dev_dataloader, test_dataloader,\\\n",
    "        learning_rate_decay=0, n_epochs=30):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    # to check the update \n",
    "    old_state_dict = {}\n",
    "    for key in model.state_dict():\n",
    "        old_state_dict[key] = model.state_dict()[key].clone()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        if learning_rate_decay != 0:\n",
    "\n",
    "            # every [decay_step] epoch reduce the learning rate by half\n",
    "            if  epoch % learning_rate_decay == 0:\n",
    "                learning_rate = learning_rate/2\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                print('at epoch {} learning_rate is updated to {}'.format(epoch, learning_rate))\n",
    "        \n",
    "        # train the model\n",
    "        losses, acc = [], []\n",
    "        label, pred = [], []\n",
    "        y_pred_col= []\n",
    "        model.train()\n",
    "        for train_data, train_label in train_dataloader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Squeeze the data [1, 33, 49], [1,5] to [33, 49], [5]\n",
    "            train_data = torch.squeeze(train_data)\n",
    "            train_label = torch.squeeze(train_label)\n",
    "            \n",
    "            # Forward pass : Compute predicted y by passing train data to the model\n",
    "            y_pred = model(train_data)\n",
    "            \n",
    "            # y_pred = y_pred[:, None]\n",
    "            # train_label = train_label[:, None]\n",
    "            \n",
    "            #print(y_pred.shape)\n",
    "            #print(train_label.shape)\n",
    "            \n",
    "            # Save predict and label\n",
    "            y_pred_col.append(y_pred.item())\n",
    "            pred.append(y_pred.item() > 0.5)\n",
    "            label.append(train_label.item())\n",
    "            \n",
    "            #print('y_pred: {}\\t label: {}'.format(y_pred, train_label))\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(y_pred, train_label)\n",
    "            acc.append(\n",
    "                torch.eq(\n",
    "                    (torch.sigmoid(y_pred).data > 0.5).float(),\n",
    "                    train_label)\n",
    "            )\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # perform a backward pass, and update the weights.\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        \n",
    "        train_acc = torch.mean(torch.cat(acc).float())\n",
    "        train_loss = np.mean(losses)\n",
    "        \n",
    "        train_pred_out = pred\n",
    "        train_label_out = label\n",
    "        \n",
    "        # save new params\n",
    "        new_state_dict= {}\n",
    "        for key in model.state_dict():\n",
    "            new_state_dict[key] = model.state_dict()[key].clone()\n",
    "            \n",
    "        # compare params\n",
    "        for key in old_state_dict:\n",
    "            if (old_state_dict[key] == new_state_dict[key]).all():\n",
    "                print('Not updated in {}'.format(key))\n",
    "   \n",
    "        \n",
    "        # dev loss\n",
    "        losses, acc = [], []\n",
    "        label, pred = [], []\n",
    "        model.eval()\n",
    "        for dev_data, dev_label in dev_dataloader:\n",
    "            # Squeeze the data [1, 33, 49], [1,5] to [33, 49], [5]\n",
    "            dev_data = torch.squeeze(dev_data)\n",
    "            dev_label = torch.squeeze(dev_label)\n",
    "            \n",
    "            # Forward pass : Compute predicted y by passing train data to the model\n",
    "            y_pred = model(dev_data)\n",
    "            \n",
    "            # Save predict and label\n",
    "            pred.append(y_pred.item())\n",
    "            label.append(dev_label.item())\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(y_pred, dev_label)\n",
    "            acc.append(\n",
    "                torch.eq(\n",
    "                    (torch.sigmoid(y_pred).data > 0.5).float(),\n",
    "                    dev_label)\n",
    "            )\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "        dev_acc = torch.mean(torch.cat(acc).float())\n",
    "        dev_loss = np.mean(losses)\n",
    "        \n",
    "        dev_pred_out = pred\n",
    "        dev_label_out = label\n",
    "        \n",
    "        # test loss\n",
    "        losses, acc = [], []\n",
    "        label, pred = [], []\n",
    "        model.eval()\n",
    "        for test_data, test_label in test_dataloader:\n",
    "            # Squeeze the data [1, 33, 49], [1,5] to [33, 49], [5]\n",
    "            test_data = torch.squeeze(test_data)\n",
    "            test_label = torch.squeeze(test_label)\n",
    "            \n",
    "            # Forward pass : Compute predicted y by passing train data to the model\n",
    "            y_pred = model(test_data)\n",
    "            \n",
    "            # Save predict and label\n",
    "            pred.append(y_pred.item())\n",
    "            label.append(test_label.item())\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(y_pred, test_label)\n",
    "            acc.append(\n",
    "                torch.eq(\n",
    "                    (torch.sigmoid(y_pred).data > 0.5).float(),\n",
    "                    test_label)\n",
    "            )\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "        test_acc = torch.mean(torch.cat(acc).float())\n",
    "        test_loss = np.mean(losses)\n",
    "        \n",
    "        test_pred_out = pred\n",
    "        test_label_out = label\n",
    "                \n",
    "        epoch_losses.append([\n",
    "             train_loss, dev_loss, test_loss,\n",
    "             train_acc, dev_acc, test_acc,\n",
    "             train_pred_out, dev_pred_out, test_pred_out,\n",
    "             train_label_out, dev_label_out, test_label_out,\n",
    "         ])\n",
    "        \n",
    "        pred = np.asarray(pred)\n",
    "        label = np.asarray(label)\n",
    "        \n",
    "        auc_score = roc_auc_score(label, pred)\n",
    "        \n",
    "        # print(\"Epoch: {} Train: {:.4f}/{:.2f}%, Dev: {:.4f}/{:.2f}%, Test: {:.4f}/{:.2f}% AUC: {:.4f}\".format(\n",
    "        #     epoch, train_loss, train_acc*100, dev_loss, dev_acc*100, test_loss, test_acc*100, auc_score))\n",
    "        print(\"Epoch: {} Train loss: {:.4f}, Dev loss: {:.4f}, Test loss: {:.4f}, Test AUC: {:.4f}\".format(\n",
    "            epoch, train_loss, dev_loss, test_loss, auc_score))\n",
    "        \n",
    "        # save the parameters\n",
    "        train_log = []\n",
    "        train_log.append(model.state_dict())\n",
    "        torch.save(model.state_dict(), './data/raw/grud_mean_grud_para.pt')\n",
    "        \n",
    "        #print(train_log)\n",
    "    \n",
    "    return epoch_losses      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "class GRUD(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, x_mean=0,\\\n",
    "                 bias=True, batch_first=False, bidirectional=False, dropout_type='mloss', dropout=0):\n",
    "        super(GRUD, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.zeros = torch.autograd.Variable(torch.zeros(input_size))\n",
    "        self.x_mean = torch.autograd.Variable(torch.tensor(x_mean))\n",
    "        self.bias = bias\n",
    "        self.batch_first = batch_first\n",
    "        self.dropout_type = dropout_type\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        if not isinstance(dropout, numbers.Number) or not 0 <= dropout <= 1 or \\\n",
    "                isinstance(dropout, bool):\n",
    "            raise ValueError(\"dropout should be a number in range [0, 1] \"\n",
    "                             \"representing the probability of an element being \"\n",
    "                             \"zeroed\")\n",
    "        if dropout > 0 and num_layers == 1:\n",
    "            warnings.warn(\"dropout option adds dropout after all but last \"\n",
    "                          \"recurrent layer, so non-zero dropout expects \"\n",
    "                          \"num_layers greater than 1, but got dropout={} and \"\n",
    "                          \"num_layers={}\".format(dropout, num_layers))\n",
    "        \n",
    "        ################################\n",
    "        gate_size = 1 # not used\n",
    "        ################################\n",
    "        \n",
    "        self._all_weights = []\n",
    "\n",
    "        '''\n",
    "        w_ih = Parameter(torch.Tensor(gate_size, layer_input_size))\n",
    "        w_hh = Parameter(torch.Tensor(gate_size, hidden_size))\n",
    "        b_ih = Parameter(torch.Tensor(gate_size))\n",
    "        b_hh = Parameter(torch.Tensor(gate_size))\n",
    "        layer_params = (w_ih, w_hh, b_ih, b_hh)\n",
    "        '''\n",
    "        # decay rates gamma\n",
    "        w_dg_x = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "        w_dg_h = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # z\n",
    "        w_xz = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "        w_hz = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        w_mz = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "\n",
    "        # r\n",
    "        w_xr = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "        w_hr = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        w_mr = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "\n",
    "        # h_tilde\n",
    "        w_xh = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "        w_hh = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        w_mh = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "\n",
    "        # y (output)\n",
    "        w_hy = torch.nn.Parameter(torch.Tensor(output_size, hidden_size))\n",
    "\n",
    "        # bias\n",
    "        b_dg_x = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b_dg_h = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b_z = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b_r = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b_h = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b_y = torch.nn.Parameter(torch.Tensor(output_size))\n",
    "\n",
    "        layer_params = (w_dg_x, w_dg_h,\\\n",
    "                        w_xz, w_hz, w_mz,\\\n",
    "                        w_xr, w_hr, w_mr,\\\n",
    "                        w_xh, w_hh, w_mh,\\\n",
    "                        w_hy,\\\n",
    "                        b_dg_x, b_dg_h, b_z, b_r, b_h, b_y)\n",
    "\n",
    "        param_names = ['weight_dg_x', 'weight_dg_h',\\\n",
    "                       'weight_xz', 'weight_hz','weight_mz',\\\n",
    "                       'weight_xr', 'weight_hr','weight_mr',\\\n",
    "                       'weight_xh', 'weight_hh','weight_mh',\\\n",
    "                       'weight_hy']\n",
    "        if bias:\n",
    "            param_names += ['bias_dg_x', 'bias_dg_h',\\\n",
    "                            'bias_z',\\\n",
    "                            'bias_r',\\\n",
    "                            'bias_h',\\\n",
    "                            'bias_y']\n",
    "        \n",
    "        for name, param in zip(param_names, layer_params):\n",
    "            setattr(self, name, param)\n",
    "        self._all_weights.append(param_names)\n",
    "\n",
    "        self.flatten_parameters()\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def flatten_parameters(self):\n",
    "        \"\"\"\n",
    "        Resets parameter data pointer so that they can use faster code paths.\n",
    "        Right now, this works only if the module is on the GPU and cuDNN is enabled.\n",
    "        Otherwise, it's a no-op.\n",
    "        \"\"\"\n",
    "        any_param = next(self.parameters()).data\n",
    "        if not any_param.is_cuda or not torch.backends.cudnn.is_acceptable(any_param):\n",
    "            return\n",
    "\n",
    "        # If any parameters alias, we fall back to the slower, copying code path. This is\n",
    "        # a sufficient check, because overlapping parameter buffers that don't completely\n",
    "        # alias would break the assumptions of the uniqueness check in\n",
    "        # Module.named_parameters().\n",
    "        all_weights = self._flat_weights\n",
    "        unique_data_ptrs = set(p.data_ptr() for p in all_weights)\n",
    "        if len(unique_data_ptrs) != len(all_weights):\n",
    "            return\n",
    "\n",
    "        with torch.cuda.device_of(any_param):\n",
    "            import torch.backends.cudnn.rnn as rnn\n",
    "\n",
    "            # NB: This is a temporary hack while we still don't have Tensor\n",
    "            # bindings for ATen functions\n",
    "            with torch.no_grad():\n",
    "                # NB: this is an INPLACE function on all_weights, that's why the\n",
    "                # no_grad() is necessary.\n",
    "                torch._cudnn_rnn_flatten_weight(\n",
    "                    all_weights, (4 if self.bias else 2),\n",
    "                    self.input_size, rnn.get_cudnn_mode(self.mode), self.hidden_size, self.num_layers,\n",
    "                    self.batch_first, bool(self.bidirectional))\n",
    "\n",
    "    def _apply(self, fn):\n",
    "        ret = super(GRUD, self)._apply(fn)\n",
    "        self.flatten_parameters()\n",
    "        return ret\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            torch.nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def check_forward_args(self, input, hidden, batch_sizes):\n",
    "        is_input_packed = batch_sizes is not None\n",
    "        expected_input_dim = 2 if is_input_packed else 3\n",
    "        if input.dim() != expected_input_dim:\n",
    "            raise RuntimeError(\n",
    "                'input must have {} dimensions, got {}'.format(\n",
    "                    expected_input_dim, input.dim()))\n",
    "        if self.input_size != input.size(-1):\n",
    "            raise RuntimeError(\n",
    "                'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n",
    "                    self.input_size, input.size(-1)))\n",
    "\n",
    "        if is_input_packed:\n",
    "            mini_batch = int(batch_sizes[0])\n",
    "        else:\n",
    "            mini_batch = input.size(0) if self.batch_first else input.size(1)\n",
    "\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        expected_hidden_size = (self.num_layers * num_directions,\n",
    "                                mini_batch, self.hidden_size)\n",
    "        \n",
    "        def check_hidden_size(hx, expected_hidden_size, msg='Expected hidden size {}, got {}'):\n",
    "            if tuple(hx.size()) != expected_hidden_size:\n",
    "                raise RuntimeError(msg.format(expected_hidden_size, tuple(hx.size())))\n",
    "\n",
    "        if self.mode == 'LSTM':\n",
    "            check_hidden_size(hidden[0], expected_hidden_size,\n",
    "                              'Expected hidden[0] size {}, got {}')\n",
    "            check_hidden_size(hidden[1], expected_hidden_size,\n",
    "                              'Expected hidden[1] size {}, got {}')\n",
    "        else:\n",
    "            check_hidden_size(hidden, expected_hidden_size)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        s = '{input_size}, {hidden_size}'\n",
    "        if self.num_layers != 1:\n",
    "            s += ', num_layers={num_layers}'\n",
    "        if self.bias is not True:\n",
    "            s += ', bias={bias}'\n",
    "        if self.batch_first is not False:\n",
    "            s += ', batch_first={batch_first}'\n",
    "        if self.dropout != 0:\n",
    "            s += ', dropout={dropout}'\n",
    "        if self.bidirectional is not False:\n",
    "            s += ', bidirectional={bidirectional}'\n",
    "        return s.format(**self.__dict__)\n",
    "    \n",
    "    \n",
    "    def __setstate__(self, d):\n",
    "        super(GRUD, self).__setstate__(d)\n",
    "        if 'all_weights' in d:\n",
    "            self._all_weights = d['all_weights']\n",
    "        if isinstance(self._all_weights[0][0], str):\n",
    "            return\n",
    "        num_layers = self.num_layers\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        self._all_weights = []\n",
    "\n",
    "        weights = ['weight_dg_x', 'weight_dg_h',\\\n",
    "                   'weight_xz', 'weight_hz','weight_mz',\\\n",
    "                   'weight_xr', 'weight_hr','weight_mr',\\\n",
    "                   'weight_xh', 'weight_hh','weight_mh',\\\n",
    "                   'weight_hy',\\\n",
    "                   'bias_dg_x', 'bias_dg_h',\\\n",
    "                   'bias_z', 'bias_r', 'bias_h','bias_y']\n",
    "\n",
    "        if self.bias:\n",
    "            self._all_weights += [weights]\n",
    "        else:\n",
    "            self._all_weights += [weights[:2]]\n",
    "\n",
    "    @property\n",
    "    def _flat_weights(self):\n",
    "        return list(self._parameters.values())\n",
    "\n",
    "    @property\n",
    "    def all_weights(self):\n",
    "        return [[getattr(self, weight) for weight in weights] for weights in self._all_weights]\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # input.size = (3, 33,49) : num_input or num_hidden, num_layer or step\n",
    "        X = torch.squeeze(input[0]) # .size = (33,49)\n",
    "        Mask = torch.squeeze(input[1]) # .size = (33,49)\n",
    "        Delta = torch.squeeze(input[2]) # .size = (33,49)\n",
    "        Hidden_State = torch.autograd.Variable(torch.zeros(input_size))\n",
    "        \n",
    "        step_size = X.size(1) # 49\n",
    "        #print('step size : ', step_size)\n",
    "        \n",
    "        output = None\n",
    "        h = Hidden_State\n",
    "\n",
    "        # decay rates gamma\n",
    "        w_dg_x = getattr(self, 'weight_dg_x')\n",
    "        w_dg_h = getattr(self, 'weight_dg_h')\n",
    "\n",
    "        #z\n",
    "        w_xz = getattr(self, 'weight_xz')\n",
    "        w_hz = getattr(self, 'weight_hz')\n",
    "        w_mz = getattr(self, 'weight_mz')\n",
    "\n",
    "        # r\n",
    "        w_xr = getattr(self, 'weight_xr')\n",
    "        w_hr = getattr(self, 'weight_hr')\n",
    "        w_mr = getattr(self, 'weight_mr')\n",
    "\n",
    "        # h_tilde\n",
    "        w_xh = getattr(self, 'weight_xh')\n",
    "        w_hh = getattr(self, 'weight_hh')\n",
    "        w_mh = getattr(self, 'weight_mh')\n",
    "\n",
    "        # bias\n",
    "        b_dg_x = getattr(self, 'bias_dg_x')\n",
    "        b_dg_h = getattr(self, 'bias_dg_h')\n",
    "        b_z = getattr(self, 'bias_z')\n",
    "        b_r = getattr(self, 'bias_r')\n",
    "        b_h = getattr(self, 'bias_h')\n",
    "        \n",
    "        for layer in range(num_layers):\n",
    "            \n",
    "            x = torch.squeeze(X[:,layer:layer+1])\n",
    "            m = torch.squeeze(Mask[:,layer:layer+1])\n",
    "            d = torch.squeeze(Delta[:,layer:layer+1])\n",
    "\n",
    "\n",
    "            #(4)\n",
    "            gamma_x = torch.exp(-torch.max(self.zeros, (w_dg_x * d + b_dg_x)))\n",
    "            gamma_h = torch.exp(-torch.max(self.zeros, (w_dg_h * d + b_dg_h)))\n",
    "\n",
    "            #(5)\n",
    "            x = m * x + (1 - m) * (gamma_x * x + (1 - gamma_x) * self.x_mean)\n",
    "\n",
    "            #(6)\n",
    "            if self.dropout == 0:\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "\n",
    "            elif self.dropout_type == 'Moon':\n",
    "                '''\n",
    "                RNNDROP: a novel dropout for rnn in asr(2015)\n",
    "                '''\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "                dropout = torch.nn.Dropout(p=self.dropout)\n",
    "                h = dropout(h)\n",
    "\n",
    "            elif self.dropout_type == 'Gal':\n",
    "                '''\n",
    "                A Theoretically grounded application of dropout in recurrent neural networks(2015)\n",
    "                '''\n",
    "                dropout = torch.nn.Dropout(p=self.dropout)\n",
    "                h = dropout(h)\n",
    "\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "\n",
    "            elif self.dropout_type == 'mloss':\n",
    "                '''\n",
    "                recurrent dropout without memory loss arXiv 1603.05118\n",
    "                g = h_tilde, p = the probability to not drop a neuron\n",
    "                '''\n",
    "\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                dropout = torch.nn.Dropout(p=self.dropout)\n",
    "                h_tilde = dropout(h_tilde)\n",
    "\n",
    "                h = (1 - z)* h + z*h_tilde\n",
    "\n",
    "            else:\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "\n",
    "            \n",
    "        w_hy = getattr(self, 'weight_hy')\n",
    "        b_y = getattr(self, 'bias_y')\n",
    "\n",
    "        output = torch.matmul(w_hy.float(), h.float()) + b_y.float()\n",
    "        output = torch.sigmoid(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_t_dataset(savepkl_folder_path, sub_folder, t_dataset_path, y_out_path):\n",
    "    savepkl_folder_path = os.path.join(savepkl_folder_path, sub_folder)\n",
    "    t_dataset_path = os.path.join(savepkl_folder_path, t_dataset_path)\n",
    "    y_out_path = os.path.join(savepkl_folder_path, y_out_path)\n",
    "\n",
    "    with open(y_out_path, 'rb') as fi:\n",
    "        y_data = pickle.load(fi)\n",
    "        y_data = y_data.reshape(len(y_data), 1)\n",
    "\n",
    "    with open(t_dataset_path, 'rb') as fi:\n",
    "        t_dataset = pickle.load(fi)\n",
    "    return t_dataset, y_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config/parameters.ini']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(os.path.pardir)\n",
    "config_path = 'config/parameters.ini'\n",
    "pars = configparser.ConfigParser(interpolation=ExtendedInterpolation())\n",
    "del pars\n",
    "pars = configparser.ConfigParser(interpolation=ExtendedInterpolation())\n",
    "pars.read(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_out_path = 'data/raw/y_out.pkl'\n",
    "# x_mean_path = 'data/raw/x_mean_aft_nor.pkl'\n",
    "# x_median_path = 'data/raw/x_median_aft_nor.pkl'\n",
    "# t_dataset_path = 'data/raw/t_dataset.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepkl_folder_path = 'data/savepkl'\n",
    "t_dataset_path = 't_dataset.pkl'\n",
    "y_out_path = 'y_out.pkl'\n",
    "x_mean_path = 'x_mean_aft_nor.pkl'\n",
    "x_median_path = 'x_median_aft_nor.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean_path = os.path.join(os.path.join(savepkl_folder_path, 'train'), x_mean_path)\n",
    "x_median_path = os.path.join(os.path.join(savepkl_folder_path, 'train'), x_median_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(x_mean_path, 'rb') as fi:\n",
    "    x_mean_aft_nor = pickle.load(fi)\n",
    "\n",
    "with open(x_median_path, 'rb') as fi:\n",
    "    x_median_aft_nor = pickle.load(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset for train \n",
    "t_dataset_train, y_train = get_t_dataset(savepkl_folder_path, 'train', t_dataset_path, y_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset for valid \n",
    "t_dataset_valid, y_valid = get_t_dataset(savepkl_folder_path, 'valid', t_dataset_path, y_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset for test \n",
    "t_dataset_test, y_test = get_t_dataset(savepkl_folder_path, 'test', t_dataset_path, y_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape : torch.Size([2922, 3, 51, 7])\t train_label.shape : torch.Size([2922, 1])\n",
      "dev_data.shape : torch.Size([731, 3, 51, 7])\t dev_label.shape : torch.Size([731, 1])\n",
      "test_data.shape : torch.Size([730, 3, 51, 7])\t test_label.shape : torch.Size([730, 1])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, dev_dataloader, test_dataloader = data_dataloader(t_dataset_train, y_train,\n",
    "                                                                   t_dataset_valid, y_valid,\n",
    "                                                                   t_dataset_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 51 # num of variables base on the paper\n",
    "hidden_size = 51 # same as inputsize\n",
    "output_size = 1\n",
    "num_layers = 7 # num of step or layers base on the paper\n",
    "\n",
    "# x_mean = torch.Tensor(np.load('./input/x_mean_aft_nor.npy'))\n",
    "# x_median = torch.Tensor(np.load('./input/x_median_aft_nor.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropout_type : Moon, Gal, mloss\n",
    "a_model = GRUD(input_size = input_size, \n",
    "               hidden_size= hidden_size,\n",
    "               output_size=output_size,\n",
    "               dropout=0, \n",
    "               dropout_type='mloss',\n",
    "               x_mean=x_mean_aft_nor,\n",
    "               num_layers=num_layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.002\n",
    "learning_rate_decay =7 \n",
    "n_epochs = 14\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 0 learning_rate is updated to 0.001\n",
      "Epoch: 0 Train loss: 0.4551, Dev loss: 0.3673, Test loss: 0.3263, Test AUC: 0.3654\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-34cdb05b3ec7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m epoch_losses = fit(a_model, criterion, learning_rate,\\\n\u001b[1;32m      3\u001b[0m                    \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                    learning_rate_decay, n_epochs)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-fc3923d75c28>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, criterion, learning_rate, train_dataloader, dev_dataloader, test_dataloader, learning_rate_decay, n_epochs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# Forward pass : Compute predicted y by passing train data to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;31m# Save predict and label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-d930689abc89>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_xz\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw_hz\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw_mz\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_xr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw_hr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw_mr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                 \u001b[0mh_tilde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_xh\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw_mh\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh_tilde\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# learning_rate = 0.1 learning_rate_decay=True\n",
    "epoch_losses = fit(a_model, criterion, learning_rate,\\\n",
    "                   train_dataloader, dev_dataloader, test_dataloader,\\\n",
    "                   learning_rate_decay, n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:airpol] *",
   "language": "python",
   "name": "conda-env-airpol-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
